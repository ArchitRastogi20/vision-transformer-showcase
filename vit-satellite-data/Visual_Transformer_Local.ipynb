{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d21c31a5",
   "metadata": {},
   "source": [
    "USING THIS AS A REFERENCE\n",
    "\n",
    "https://www.geeksforgeeks.org/deep-learning/building-a-vision-transformer-from-scratch-in-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c6f703",
   "metadata": {},
   "source": [
    "Dataset used:\n",
    "https://www.kaggle.com/datasets/mahmoudreda55/satellite-image-classification/data\n",
    "\n",
    "Satellite image Classification Dataset-RSI-CB256 , This dataset has 4 different classes mixed from Sensors and google map snapshot\n",
    "Labels and quantities:\n",
    "Cloudy 1500\n",
    "Desert 1131\n",
    "Green_Area 1500\n",
    "Water 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e92a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import psutil\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Import other key packages safely\n",
    "try:\n",
    "    import timm\n",
    "except ImportError:\n",
    "    timm = None\n",
    "\n",
    "try:\n",
    "    import safetensors\n",
    "except ImportError:\n",
    "    safetensors = None\n",
    "\n",
    "try:\n",
    "    import sklearn\n",
    "except ImportError:\n",
    "    sklearn = None\n",
    "\n",
    "try:\n",
    "    import numpy\n",
    "except ImportError:\n",
    "    numpy = None\n",
    "\n",
    "\n",
    "def get_windows_version():\n",
    "    if platform.system() == \"Windows\":\n",
    "        build = sys.getwindowsversion().build\n",
    "        if build >= 22000:\n",
    "            return \"Windows 11\"\n",
    "        else:\n",
    "            return \"Windows 10 or earlier\"\n",
    "    else:\n",
    "        return platform.system()\n",
    "\n",
    "\n",
    "def get_detailed_cpu_info():\n",
    "    info = {}\n",
    "\n",
    "    # CPU brand and architecture\n",
    "    info['Processor'] = platform.processor()\n",
    "    info['Machine'] = platform.machine()\n",
    "\n",
    "    # Physical cores\n",
    "    info['Physical cores'] = psutil.cpu_count(logical=False)\n",
    "\n",
    "    # Logical processors (threads)\n",
    "    info['Logical processors'] = psutil.cpu_count(logical=True)\n",
    "\n",
    "    # CPU frequency info per core\n",
    "    freq = psutil.cpu_freq(percpu=True)\n",
    "    if freq:\n",
    "        info['CPU frequency per core (MHz)'] = [f.current for f in freq]\n",
    "    else:\n",
    "        info['CPU frequency per core (MHz)'] = None\n",
    "\n",
    "    # Average CPU frequency\n",
    "    avg_freq = psutil.cpu_freq()\n",
    "    info['Average CPU frequency (MHz)'] = avg_freq.current if avg_freq else None\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "def get_system_info():\n",
    "    info = get_detailed_cpu_info()\n",
    "\n",
    "    # RAM information (GB)\n",
    "    mem = psutil.virtual_memory()\n",
    "    info['RAM_GB'] = round(mem.total / (1024 ** 3), 2)\n",
    "\n",
    "    # Operating System details with accurate Windows version detection\n",
    "    info['OS_System'] = platform.system()\n",
    "    info['OS_Version'] = get_windows_version() if info['OS_System'] == \"Windows\" else platform.version()\n",
    "    info['OS_Node'] = platform.node()\n",
    "    info['OS_Release'] = platform.release()\n",
    "    info['OS_Machine'] = platform.machine()\n",
    "    info['OS_Processor'] = platform.processor()\n",
    "\n",
    "    # Python version\n",
    "    info['Python_Version'] = platform.python_version()\n",
    "\n",
    "    # Package versions where available\n",
    "    info['PyTorch_Version'] = torch.__version__\n",
    "    info['Timm_Version'] = timm.__version__ if timm else \"Not installed\"\n",
    "    info['Safetensors_Version'] = safetensors.__version__ if safetensors else \"Not installed\"\n",
    "    info['Scikit-learn_Version'] = sklearn.__version__ if sklearn else \"Not installed\"\n",
    "    info['NumPy_Version'] = numpy.__version__ if numpy else \"Not installed\"\n",
    "\n",
    "    # CUDA info\n",
    "    info['CUDA_Available'] = torch.cuda.is_available()\n",
    "    info['CUDA_Version'] = torch.version.cuda if torch.cuda.is_available() else None\n",
    "    info['GPU_Name'] = torch.cuda.get_device_name(0) if torch.cuda.is_available() else None\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    system_info = get_system_info()\n",
    "    for k, v in system_info.items():\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe0d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Batch size, Channels, Height, Width of the input tensor\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c254551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Adding Positional Embeddings\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, seq_len):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, seq_len + 1, embed_dim))  # Adjusted for [CLS] token\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, S, E) where\n",
    "        # B = batch size,\n",
    "        # S = sequence length (number of patches or tokens),\n",
    "        # E = embedding dimension (feature size per token)\n",
    "        out, _ = self.attn(x, x, x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f1550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Apply LayerNorm then Multi-Head Self-Attention to capture contextual relationships across tokens\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        # Residual connection adds input back after attention\n",
    "\n",
    "        # Apply LayerNorm then position-wise Feed-Forward Network (MLP) for non-linear token-wise transformation\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        # Residual connection again\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "'''\n",
    "GELU attempt that didn't yield good results\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(0.1)\n",
    ")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe884ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, num_classes=10, embed_dim=768, num_heads=8, depth=6, mlp_dim=1024):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, (img_size // patch_size) ** 2)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, mlp_dim) for _ in range(depth)\n",
    "        ])\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.mlp_head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embedding(x)  # shape: (B, num_patches, embed_dim)\n",
    "        \n",
    "        ##### Added by Archit\n",
    "        if x.size(1) != self.pos_encoding.pos_embed.size(1): pos_embed = self.pos_encoding.pos_embed[:, :x.size(1), :]\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, embed_dim)\n",
    "\n",
    "        # Adjust positional encoding to input sequence length\n",
    "        if x.size(1) != self.pos_encoding.pos_embed.size(1):\n",
    "            pos_embed = self.pos_encoding.pos_embed[:, :x.size(1), :]\n",
    "        else:\n",
    "            pos_embed = self.pos_encoding.pos_embed\n",
    "\n",
    "        x = x + pos_embed\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        return self.mlp_head(x[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "\n",
    "\n",
    "# Parameters (originally 32 / 72 / 128 / 0.2)\n",
    "# a ViT expects images of a certain size (e.g., 224x224), yielding a fixed number of patches (197 includes 196 patches + 1 CLS token)\n",
    "path = \"satellite_data/\"\n",
    "batch_size = 32\n",
    "height = 224\n",
    "width = 224\n",
    "val_split_ratio = 0.2\n",
    "\n",
    "\n",
    "# Augmentation transforms for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((height, width)),           # Resize to your specified size\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),       # Random horizontal flip\n",
    "    #transforms.RandomVerticalFlip(p=0.5),         # Random vertical flip\n",
    "    #transforms.RandomRotation(degrees=30),        # Random rotation within Â±30 degrees (turned off because resolution too low)\n",
    "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "# Validation transforms (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((height, width)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "# Load the full dataset without transforms first\n",
    "full_dataset = datasets.ImageFolder(root=path)\n",
    "\n",
    "\n",
    "# Calculate split lengths\n",
    "val_size = int(len(full_dataset) * val_split_ratio)\n",
    "train_size = len(full_dataset) - val_size\n",
    "\n",
    "\n",
    "# Indices for train and validation splits\n",
    "train_indices = list(range(train_size))\n",
    "val_indices = list(range(train_size, train_size + val_size))\n",
    "\n",
    "\n",
    "# Create subsets for train and validation\n",
    "train_subset = Subset(full_dataset, train_indices)\n",
    "val_subset = Subset(full_dataset, val_indices)\n",
    "\n",
    "\n",
    "# Dataset class to apply transform dynamically\n",
    "class AugmentedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx]\n",
    "        x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "# Wrapped datasets with appropriate transforms\n",
    "train_dataset = AugmentedDataset(train_subset, train_transform)\n",
    "val_dataset = AugmentedDataset(val_subset, val_transform)\n",
    "\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d80172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure augmentation is working properly by showing random images from augmented dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def show_augmented_images(dataset, n=5):\n",
    "    fig, axes = plt.subplots(1, n, figsize=(15, 5))\n",
    "    for i in range(n):\n",
    "        img, label = dataset[i]\n",
    "        # Convert tensor to numpy image with proper normalization reversal for display\n",
    "        img_show = img.permute(1, 2, 0).numpy()\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        img_show = std * img_show + mean\n",
    "        img_show = img_show.clip(0, 1)\n",
    "        axes[i].imshow(img_show)\n",
    "        axes[i].set_title(f'Label: {label}')\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_augmented_images(train_dataset, n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b87c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This code is the original locally trained and should be run at standard resolution (not 224x224)\n",
    "# for replication of the results make sure the pipiline is using standard resolution (written in a comment)\n",
    "\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "##### Parameters:\n",
    "##### original: lr 0.001 / epoch 5 / batch_size 32\n",
    "lr = 3e-4\n",
    "epoch_count = 20\n",
    "batch_size = 32\n",
    "weight_decay = 0.05\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = train_dataset\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "\n",
    "model = VisionTransformer()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "\n",
    "# Start total timer\n",
    "total_start_time = time.time()\n",
    "\n",
    "# To be used later for graphs\n",
    "class TrainingHistory:\n",
    "    def __init__(self):\n",
    "        # Initialize with epoch 0 values all set to zero\n",
    "        self.losses = [0.0]\n",
    "        self.accuracies = [0.0]\n",
    "        self.precisions = [0.0]\n",
    "        self.recalls = [0.0]\n",
    "        self.f1s = [0.0]\n",
    "        self.durations = [0.0]\n",
    "\n",
    "# Create history object before training\n",
    "history = TrainingHistory()\n",
    "\n",
    "# ensure consistency in epoch count, use the same digits count so that i have formatted outputs\n",
    "# i didn't bother running it with 100 epoch count, but it does work with 1 to 20 epochs\n",
    "width = len(str(epoch_count))\n",
    "\n",
    "print(f\"Epoch [{0:0{width}d}/{epoch_count}] \"\n",
    "      f\"Loss: {0.0:.4f} Accuracy: {0.0:.4f} Precision: {0.0:.4f} \"\n",
    "      f\"Recall: {0.0:.4f} F1-score: {0.0:.4f} \"\n",
    "      f\"Epoch Time: {0.00:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epoch_count):  # Train for epoch_count epochs\n",
    "    epoch_start_time = time.time()  # Start timer for this epoch\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    accuracy = (torch.tensor(all_preds) == torch.tensor(all_labels)).float().mean().item()\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    epoch_end_time = time.time()  # End timer for this epoch\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "\n",
    "    # Store metrics and duration in history object\n",
    "    history.losses.append(epoch_loss)\n",
    "    history.accuracies.append(accuracy)\n",
    "    history.precisions.append(precision)\n",
    "    history.recalls.append(recall)\n",
    "    history.f1s.append(f1)\n",
    "    history.durations.append(epoch_duration)\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch+1:0{width}d}/{epoch_count}] \"\n",
    "        f\"Loss: {epoch_loss:.4f} Accuracy: {accuracy:.4f} Precision: {precision:.4f} \"\n",
    "        f\"Recall: {recall:.4f} F1-score: {f1:.4f} \"\n",
    "        f\"Epoch Time: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "\n",
    "# End total timer\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "\n",
    "print(f\"Total Training Time: {total_duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f68792",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### HYBRID VERSION, using many parts of the final pretrained model so that they can be both tested in the same conditions\n",
    "# make sure this one is using 224x244 images to test 1:1 with the pretrained, check transforms in the above cells\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "import numpy as np\n",
    "\n",
    "##### Parameters:\n",
    "lr = 3e-4\n",
    "epoch_count = 20\n",
    "batch_size = 16\n",
    "weight_decay = 0.05\n",
    "num_classes = 4\n",
    "\n",
    "print(f\"Dataset size: {len(train_dataset)} images\")\n",
    "\n",
    "# OPTIMIZED DATALOADER\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pretrained Vision Transformer\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Performance optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "# Start total timer\n",
    "total_start_time = time.time()\n",
    "\n",
    "# To be used later for graphs\n",
    "class TrainingHistory:\n",
    "    def __init__(self):\n",
    "        self.losses = [0.0]\n",
    "        self.accuracies = [0.0]\n",
    "        self.precisions = [0.0]\n",
    "        self.recalls = [0.0]\n",
    "        self.f1s = [0.0]\n",
    "        self.durations = [0.0]\n",
    "\n",
    "# Create history object before training\n",
    "history = TrainingHistory()\n",
    "\n",
    "# ensure consistency in epoch count, use the same digits count so that i have formatted outputs\n",
    "width = len(str(epoch_count))\n",
    "\n",
    "print(f\"Epoch [{0:0{width}d}/{epoch_count}] \"\n",
    "      f\"Loss: {0.0:.4f} Accuracy: {0.0:.4f} Precision: {0.0:.4f} \"\n",
    "      f\"Recall: {0.0:.4f} F1-score: {0.0:.4f} \"\n",
    "      f\"Epoch Time: {0.00:.2f} seconds\")\n",
    "\n",
    "# MAIN TRAINING LOOP\n",
    "for epoch in range(epoch_count):  # Train for epoch_count epochs\n",
    "    epoch_start_time = time.time()  # Start timer for this epoch\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    accuracy = (torch.tensor(all_preds) == torch.tensor(all_labels)).float().mean().item()\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    epoch_end_time = time.time()  # End timer for this epoch\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "\n",
    "    # Store metrics and duration in history object\n",
    "    history.losses.append(epoch_loss)\n",
    "    history.accuracies.append(accuracy)\n",
    "    history.precisions.append(precision)\n",
    "    history.recalls.append(recall)\n",
    "    history.f1s.append(f1)\n",
    "    history.durations.append(epoch_duration)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1:0{width}d}/{epoch_count}] \"\n",
    "          f\"Loss: {epoch_loss:.4f} Accuracy: {accuracy:.4f} Precision: {precision:.4f} \"\n",
    "          f\"Recall: {recall:.4f} F1-score: {f1:.4f} \"\n",
    "          f\"Epoch Time: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "# End total timer\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "\n",
    "print(f\"Total Training Time: {total_duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8065f",
   "metadata": {},
   "source": [
    "To better understand the metrics used:\n",
    "\n",
    "Loss: Measures the model's error in prediction by quantifying how far the predictions are from the true labels using the loss function (in this case cross-entropy). Lower loss indicates better fit to training data. Loss is crucial during training as it is the value optimized by the model's algorithm to improve performance\n",
    "(The lower the better)\n",
    "\n",
    "Accuracy: Represents the proportion of correct predictions out of all predictions made. It's a simple and intuitive measure of overall correctness but can be misleading when classes training data is imbalanced in the ratio (in this dataset it's fairly even, except for desert which has roughly 80% of the data count)\n",
    "Cloudy 1500 / Desert 1131 / Green_Area 1500 / Water 1500\n",
    "\n",
    "Precision: Indicates how many predicted positive cases were actually positive. This metric is especially important when false positives are costly, such as in medical diagnoses or fraud detection, ensuring that positive predictions are reliable\n",
    "\n",
    "Recall: Measures how many actual positive cases were correctly identified. It's critical in scenarios where missing positive cases has a high cost (false negatives), like disease screening, emphasizing sensitivity\n",
    "\n",
    "F1-score: The harmonic mean of precision and recall, providing a balanced metric especially valuable when you need to account for both false positives and false negatives, and when class distribution is uneven. It summarizes model performance in one number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00cd316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "# use history (from the earlier model's training results) to compute graphs\n",
    "epochs = range(0, len(history.losses))\n",
    "\n",
    "\n",
    "\n",
    "# Plot Loss (Loss may not cap at 1.0 but for consistency, set max to 1.0)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, history.losses, 'r-', label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(0.1))  \n",
    "plt.xlim(0, epoch_count)\n",
    "plt.ylim(0, 1.0)  # y-axis max 1.0\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, history.accuracies, 'b-', label='Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "plt.xlim(0, epoch_count)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()\n",
    "\n",
    "# Plot Precision, Recall, and F1-score together\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, history.precisions, 'g-', label='Precision')\n",
    "plt.plot(epochs, history.recalls, 'm-', label='Recall')\n",
    "plt.plot(epochs, history.f1s, 'c-', label='F1-score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision, Recall, F1-score Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "plt.xlim(0, epoch_count)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot Epoch Duration\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, history.durations, 'k-', label='Epoch Duration (seconds)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Duration (s)')\n",
    "plt.title('Epoch Duration Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))  # set tick interval to 1\n",
    "plt.xlim(0, epoch_count)\n",
    "plt.ylim(bottom=0) # start y-axis at 0\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# After the epoch evaluation (after collecting all_preds and all_labels)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "\n",
    "epoch_loss = running_loss / len(train_loader)\n",
    "accuracy = (torch.tensor(all_preds) == torch.tensor(all_labels)).float().mean().item()\n",
    "precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "# New code for confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(f\"Epoch [{epoch+1}/{epoch+1}] Loss: {epoch_loss:.4f} Accuracy: {accuracy:.4f} Precision: {precision:.4f} Recall: {recall:.4f} F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aa6224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming 'all_labels' and 'all_preds' contain true and predicted labels respectively\n",
    "labels_names = ['Cloudy', 'Desert', 'Green_Area', 'Water']\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "\n",
    "# Labeling the axes with class names\n",
    "num_classes = cm.shape[0]\n",
    "plt.xticks(np.arange(num_classes), labels=labels_names, rotation=45)\n",
    "plt.yticks(np.arange(num_classes), labels=labels_names)\n",
    "\n",
    "# Axis labels\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "\n",
    "# Adding counts on the plot\n",
    "thresh = cm.max() / 2\n",
    "for i in range(num_classes):\n",
    "    for j in range(num_classes):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c55348",
   "metadata": {},
   "source": [
    "From this confusion matrix we can confidently say there's a bit of error when predicting labels 2 and 3 (Green_Area and Water), labels 0 and 1 show a similar problem (Cloudy and Desert), but not in a very accentuated way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321cbd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### for testing purposes only\n",
    "\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "# full_dataset is ImageFolder with transform=transform (might convert images, so ignore it here)\n",
    "\n",
    "# Map from class index to class name\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "# Store counts of resolutions per class\n",
    "resolutions_per_class = {cls: defaultdict(int) for cls in class_names}\n",
    "\n",
    "# Iterate over dataset samples (image paths are in dataset.imgs or dataset.samples)\n",
    "for img_path, label in full_dataset.samples:\n",
    "    with Image.open(img_path) as img:\n",
    "        # Get image size (width, height)\n",
    "        size = img.size\n",
    "    class_name = class_names[label]\n",
    "    # Increment count for this resolution for this class\n",
    "    resolutions_per_class[class_name][size] += 1\n",
    "\n",
    "# Print summary for each class\n",
    "for cls, res_counts in resolutions_per_class.items():\n",
    "    print(f\"Class '{cls}':\")\n",
    "    for res, count in sorted(res_counts.items()):\n",
    "        print(f\"  Resolution {res[0]}x{res[1]} : {count} images\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51b6d0",
   "metadata": {},
   "source": [
    "Results in locally trained baseline (before data augmentation):\n",
    "\n",
    "From the results from being run on a 4060 Mobile GPU we can confidently say that the model shows marginal changes after the 10th epoch, but usually goes from 90 to 92.5% accuracy at around 13th epoch, it might actually be counterproductive to run the model for longer since it seems to degenerate into slightly worse performance\n",
    "\n",
    "Results in locally trained with data augmentation:\n",
    "\n",
    "Precision down from 90/92% to 88%, loss is consistently the same throughout the whole train sequence (0.35 average on augmented, while goes down to 0.2 in baseline), this is suggesting that something is wrong with augmentation, will require further testing\n",
    "Tried disabling roation (having a 64x64 images there isn't much to rotate at all) and it had no effect, but maybe testing for longer (50 epochs would? we are training at twice the speed, it's only fair to run the model for twice as long to give it the same time to shine)\n",
    "Apparently not, doesn't even touch the 90% accuracy, and something odd happened at epoch 37, train time jumped from 20 to 28.8s for no reason\n",
    "Epoch [37/50] Loss: 0.3286 Accuracy: 0.8744 Precision: 0.8667 Recall: 0.8744 F1-score: 0.8648 Epoch Time: 28.82 seconds\n",
    "Dataloades have workers set at 4\n",
    "\n",
    "Resizing to 244x244 doesn't help, it only slows down the model and needs 70s on average for each epoch, turning off flipping and general augmentation\n",
    "Kept jitter on since it should be only a minimal touch with a deep effect, it's supposed to scramble things off a bit, but it might be a problem on purely color based datasets? as usual tests indicate that accuracy goes up to 89% and then down to 87%, turning off the data augmentation for another test\n",
    "Noticed that i forgot to add \" transforms.CenterCrop(224), \" to my transform, which means that this might have been compromising my results, testing normal usage without it to confirm it aligns with baseline\n",
    "\n",
    "ODD behavior i didn't foresee:\n",
    "Turning off that data transform actually helped, so augmentation actually hurt the performance enough to push it much further down than expected. Results from baseline without center crop\n",
    "Epoch [20/20] Loss: 0.1612 Accuracy: 0.9505 Precision: 0.9499 Recall: 0.9505 F1-score: 0.9484 Epoch Time: 14.48 seconds\n",
    "Perhaps a 20 epoch was too little, 50 epochs trial:\n",
    "Epoch [42/50] Loss: 0.1161 Accuracy: 0.9607 Precision: 0.9599 Recall: 0.9607 F1-score: 0.9598 Epoch Time: 14.32 seconds\n",
    "Epoch [50/50] Loss: 0.1808 Accuracy: 0.9347 Precision: 0.9325 Recall: 0.9347 F1-score: 0.9326 Epoch Time: 14.02 seconds\n",
    "96% is quite the result given the dataset, but the model tends to go back and forth and show a bit of instability on the long run, dipping into the 88% territory:\n",
    "Epoch [27/50] Loss: 0.3760 Accuracy: 0.8883 Precision: 0.8843 Recall: 0.8883 F1-score: 0.8851 Epoch Time: 14.60 seconds\n",
    "\n",
    "Swapped my RELU for GELU:\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(0.1)\n",
    ")\n",
    "Apparently it yields slightly worse results, so RELU stays\n",
    "Epoch [16/20] Loss: 0.1726 Accuracy: 0.9458 Precision: 0.9445 Recall: 0.9458 F1-score: 0.9438 Epoch Time: 14.40 seconds\n",
    "\n",
    "Labels and quantities:\n",
    "Cloudy 1500\n",
    "Desert 1131\n",
    "Green_Area 1500\n",
    "Water 1500\n",
    "\n",
    "Confirmed through dataset looping:\n",
    "Class 'cloudy':  Resolution 256x256 : 1500 images\n",
    "Class 'desert':  Resolution 256x256 : 1131 images\n",
    "Class 'green_area':  Resolution 64x64 : 1500 images\n",
    "Class 'water':  Resolution 64x64 : 1500 images\n",
    "\n",
    "A Visual Transformer tends to be data hungry, so perhaps there's an inherent flaw in attempting this with a small dataset with small images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
